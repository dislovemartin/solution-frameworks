name: "task_router"
platform: "pytorch_libtorch"
max_batch_size: 8
input [
    {
    name: "preprocessed_input_ids"
    data_type: TYPE_INT64
    dims: [ 512 ]
    },
    {
    name: "preprocessed_attention_mask"
    data_type: TYPE_INT64
    dims: [ 512 ]
    }
]

output [
    {
    name: "logits"
    data_type: TYPE_FP32
    dims:  [-1]
    }
]

instance_group [
    {
    kind: KIND_GPU
    count: 1
    }
]