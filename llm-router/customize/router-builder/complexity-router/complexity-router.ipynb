{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Router "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will explore NVIDIA's prompt classification model that analyzes prompts based on the complexity of the router. The model serves as an intelligent router for directing prompts to appropriate processing pipelines.\n",
    "\n",
    "## Model Information\n",
    "* Source: [NVIDIA's prompt-task-and-complexity-classifier](https://huggingface.co/nvidia/prompt-task-and-complexity-classifier)\n",
    "* Architecture: DeBERTa-v3-base backbone\n",
    "* Purpose: Multi-task classification for prompt analysis\n",
    "* Output: Task types and multiple complexity metrics\n",
    "\n",
    "## Complexity Categories\n",
    "The model classifies prompts into various complexity types\n",
    "\n",
    "![complexity_types](assets/Prompt_Compleity.png \"Complexity Type Categories\")\n",
    "\n",
    "* Creativity: The level of creativity needed to respond to a prompt.\n",
    "* Reasoning: The extent of logical or cognitive effort required to respond to a prompt.\n",
    "* Contextual Knowledge: The background information necessary to respond to a prompt.\n",
    "* Domain Knowledge: The amount of specialized knowledge or expertise within a specific subject area needed to respond to a prompt. \n",
    "* Constraints: The number of constraints or conditions provided with the prompt. \n",
    "* Number of Few Shots: The number of examples provided with the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        )\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "\n",
    "class MulticlassHead(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MulticlassHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CustomModel(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, target_sizes, task_type_map, weights_map, divisor_map):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(\"microsoft/DeBERTa-v3-base\")\n",
    "        self.target_sizes = target_sizes.values()\n",
    "        self.task_type_map = task_type_map\n",
    "        self.weights_map = weights_map\n",
    "        self.divisor_map = divisor_map\n",
    "\n",
    "        self.heads = [\n",
    "            MulticlassHead(self.backbone.config.hidden_size, sz)\n",
    "            for sz in self.target_sizes\n",
    "        ]\n",
    "\n",
    "        for i, head in enumerate(self.heads):\n",
    "            self.add_module(f\"head_{i}\", head)\n",
    "\n",
    "        self.pool = MeanPooling()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        mean_pooled_representation = self.pool(last_hidden_state, attention_mask)\n",
    "\n",
    "        logits = [\n",
    "            self.heads[k](mean_pooled_representation)\n",
    "            for k in range(len(self.target_sizes))\n",
    "        ]\n",
    "\n",
    "        # return self.process_logits(logits)\n",
    "        return logits\n",
    "\n",
    "class LogitsProcessor:\n",
    "    def __init__(self, task_type_map, weights_map, divisor_map):\n",
    "        self.task_type_map = task_type_map\n",
    "        self.weights_map = weights_map\n",
    "        self.divisor_map = divisor_map\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.targets = [\n",
    "            \"task_type\", \"creativity_scope\", \"reasoning\", \"contextual_knowledge\",\n",
    "            \"number_of_few_shots\", \"domain_knowledge\", \"no_label_reason\", \"constraint_ct\"\n",
    "        ]\n",
    "\n",
    "    def compute_results(self, preds, target, decimal=4):\n",
    "        if target == \"task_type\":\n",
    "            task_type = {}\n",
    "\n",
    "            top2_indices = torch.topk(preds, k=2, dim=1).indices\n",
    "            softmax_probs = torch.softmax(preds, dim=1)\n",
    "            top2_probs = softmax_probs.gather(1, top2_indices)\n",
    "            top2 = top2_indices.detach().cpu().tolist()\n",
    "            top2_prob = top2_probs.detach().cpu().tolist()\n",
    "\n",
    "            top2_strings = [\n",
    "                [self.task_type_map[str(idx)] for idx in sample] for sample in top2\n",
    "            ]\n",
    "            top2_prob_rounded = [\n",
    "                [round(value, 3) for value in sublist] for sublist in top2_prob\n",
    "            ]\n",
    "\n",
    "            counter = 0\n",
    "            for sublist in top2_prob_rounded:\n",
    "                if sublist[1] < 0.1:\n",
    "                    top2_strings[counter][1] = \"NA\"\n",
    "                counter += 1\n",
    "\n",
    "            task_type_1 = [sublist[0] for sublist in top2_strings]\n",
    "            task_type_2 = [sublist[1] for sublist in top2_strings]\n",
    "            task_type_prob = [sublist[0] for sublist in top2_prob_rounded]\n",
    "\n",
    "            return (task_type_1, task_type_2, task_type_prob)\n",
    "        else:\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "\n",
    "            weights = np.array(self.weights_map[target])\n",
    "            weighted_sum = np.sum(np.array(preds.detach().cpu()) * weights, axis=1)\n",
    "            scores = weighted_sum / self.divisor_map[target]\n",
    "\n",
    "            scores = [round(value, decimal) for value in scores]\n",
    "            if target == \"number_of_few_shots\":\n",
    "                scores = [x if x >= 0.05 else 0 for x in scores]\n",
    "            return scores\n",
    "\n",
    "    def process_logits(self, logits):\n",
    "        result = {}\n",
    "\n",
    "        for i, target in enumerate(self.targets):\n",
    "            logits_tensor = torch.from_numpy(logits[i]).float()\n",
    "            \n",
    "            if target == \"task_type\":\n",
    "                task_type_results = self.compute_results(logits_tensor, target=target)\n",
    "                result[\"task_type_1\"] = task_type_results[0]\n",
    "                result[\"task_type_2\"] = task_type_results[1]\n",
    "                result[\"task_type_prob\"] = task_type_results[2]\n",
    "            else:\n",
    "                result[target] = self.compute_results(logits_tensor, target=target)\n",
    "\n",
    "        # Calculate prompt_complexity_score\n",
    "        result[\"prompt_complexity_score\"] = [\n",
    "            round(\n",
    "                0.35 * creativity\n",
    "                + 0.25 * reasoning\n",
    "                + 0.15 * constraint\n",
    "                + 0.15 * domain_knowledge\n",
    "                + 0.05 * contextual_knowledge\n",
    "                + 0.05 * few_shots,\n",
    "                5,\n",
    "            )\n",
    "            for creativity, reasoning, constraint, domain_knowledge, contextual_knowledge, few_shots in zip(\n",
    "                result[\"creativity_scope\"],\n",
    "                result[\"reasoning\"],\n",
    "                result[\"constraint_ct\"],\n",
    "                result[\"domain_knowledge\"],\n",
    "                result[\"contextual_knowledge\"],\n",
    "                result[\"number_of_few_shots\"],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nvidia/prompt-task-and-complexity-classifier\"\n",
    ")\n",
    "model = CustomModel(\n",
    "    target_sizes=config.target_sizes,\n",
    "    task_type_map=config.task_type_map,\n",
    "    weights_map=config.weights_map,\n",
    "    divisor_map=config.divisor_map,\n",
    ").from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "model.eval()\n",
    "\n",
    "prompt = [\"Prompt: Write a Python script that uses a for loop.\"]\n",
    "\n",
    "encoded_texts = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "result = model(encoded_texts)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Router Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the model returns a list of logits from multiple classification heads, which isn't ideal for\n",
    "Triton inference server deployment, In order to address the need to handle multiple output tensors from NVIDIA's prompt classifier model we need to creating a wrapper that concatenates the outputs into a single tensor.\n",
    "\n",
    "### WrapperModel Class\n",
    "* Takes the original model as input\n",
    "* Concatenates multiple output tensors into a single tensor\n",
    "* Simplifies the output format for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "\n",
    "class TracedModel(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, target_sizes, task_type_map, weights_map, divisor_map):\n",
    "        super(TracedModel, self).__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(\"microsoft/DeBERTa-v3-base\")\n",
    "        self.target_sizes = target_sizes.values()\n",
    "        self.task_type_map = task_type_map\n",
    "        self.weights_map = weights_map\n",
    "        self.divisor_map = divisor_map\n",
    "\n",
    "        self.heads = [\n",
    "            MulticlassHead(self.backbone.config.hidden_size, sz)\n",
    "            for sz in self.target_sizes\n",
    "        ]\n",
    "\n",
    "        for i, head in enumerate(self.heads):\n",
    "            self.add_module(f\"head_{i}\", head)\n",
    "\n",
    "        self.pool = MeanPooling()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        mean_pooled_representation = self.pool(last_hidden_state, attention_mask)\n",
    "\n",
    "        logits = [\n",
    "            self.heads[k](mean_pooled_representation)\n",
    "            for k in range(len(self.target_sizes))\n",
    "        ]\n",
    "        return logits\n",
    "\n",
    "class WrapperModel(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super().__init__()\n",
    "        self.model = original_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "model = TracedModel(\n",
    "    target_sizes=config.target_sizes,\n",
    "    task_type_map=config.task_type_map,\n",
    "    weights_map=config.weights_map,\n",
    "    divisor_map=config.divisor_map,\n",
    ").from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "wrapped_model = WrapperModel(model)\n",
    "wrapped_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a Python script that uses a for loop.\"\n",
    "encoded_texts = tokenizer(\n",
    "    [prompt],\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace the wrapped model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    wrapped_model = torch.jit.trace(\n",
    "        wrapped_model,\n",
    "        (\n",
    "            encoded_texts[\"input_ids\"].to('cuda'),\n",
    "            encoded_texts[\"attention_mask\"].to('cuda')\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model.save(\"triton_template/complexity_router/1/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model and compare it with original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "\n",
    "# Load the traced model\n",
    "wrapped_model = torch.jit.load('triton_template/complexity_router/1/model.pt')\n",
    "wrapped_model.eval()\n",
    "\n",
    "# Prepare a sample input\n",
    "sample_text = \"Prompt: Translate the following sentence from English to French: 'Hello, how are you?'\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Move inputs to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = wrapped_model(input_ids, attention_mask)\n",
    "\n",
    "# Process the output\n",
    "def process_results(output_tensor, target_sizes):\n",
    "    results = []\n",
    "    start_idx = 0\n",
    "    for size in target_sizes:\n",
    "        end_idx = start_idx + size\n",
    "        result = output_tensor[:, start_idx:end_idx]\n",
    "        results.append(result)\n",
    "        start_idx = end_idx\n",
    "    return results\n",
    "\n",
    "processed_results = process_results(output, config.target_sizes.values())\n",
    "\n",
    "# Interpret the results\n",
    "task_names = list(config.target_sizes.keys())\n",
    "for i, result in enumerate(processed_results):\n",
    "    probabilities = torch.softmax(result, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0, predicted_class].item()\n",
    "    print(f\"{task_names[i]}: Predicted class = {predicted_class}, Confidence = {confidence:.4f}\")\n",
    "\n",
    "# Compare with the original model (optional)\n",
    "original_model = TracedModel(\n",
    "    target_sizes=config.target_sizes,\n",
    "    task_type_map=config.task_type_map,\n",
    "    weights_map=config.weights_map,\n",
    "    divisor_map=config.divisor_map,\n",
    ").from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "original_model = original_model.to(device)\n",
    "original_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_outputs = original_model(input_ids, attention_mask)\n",
    "    wrapped_original_output = torch.cat(original_outputs, dim=1)\n",
    "\n",
    "print(\"\\nComparing outputs:\")\n",
    "print(\"Original model output shape:\", wrapped_original_output.shape)\n",
    "print(\"Traced model output shape:\", output.shape)\n",
    "print(\"Outputs match:\", torch.allclose(wrapped_original_output, output, atol=1e-4))\n",
    "\n",
    "separated_original_outputs = process_results(wrapped_original_output, config.target_sizes.values())\n",
    "\n",
    "# Compare separated outputs\n",
    "print(\"\\nComparing separated outputs:\")\n",
    "for i, (original, traced) in enumerate(zip(separated_original_outputs, processed_results)):\n",
    "    print(f\"Task {i}:\")\n",
    "    print(f\"  Original output shape: {original.shape}\")\n",
    "    print(f\"  Traced output shape: {traced.shape}\")\n",
    "    print(f\"  Outputs match: {torch.allclose(original, traced, atol=1e-4)}\")\n",
    "\n",
    "# Interpret the original model results\n",
    "print(\"\\nOriginal model results:\")\n",
    "for i, result in enumerate(separated_original_outputs):\n",
    "    probabilities = torch.softmax(result, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0, predicted_class].item()\n",
    "    print(f\"{task_names[i]}: Predicted class = {predicted_class}, Confidence = {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_original_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the traced model in the torch script, we can add this to the [Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/contents.html) and use the [ensemble pipeline feature](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/ensemble_models.html) to set up the pre and post-processing pipeline. \n",
    "\n",
    "The pre and post-processing code is available under the `triton_template/preprocessing_compleity_router/` and `triton_template/postprocessing_complexity_router/` directories and the `triton_template/complexity_router_ensemble/` contains the config on how the pre-processing, model and post-processing are linked together. \n",
    "\n",
    "This will be the same as the code downloaded from NGC when setting up the default task router.\n",
    "\n",
    "This is organized in the following structure in the `/routers` directory with the following format\n",
    "\n",
    "```\n",
    "model_repository/\n",
    "├── complexity_router\n",
    "│   ├── 1\n",
    "│   │   └── model.pt\n",
    "│   └── config.pbtxt\n",
    "├── complexity_router_ensemble\n",
    "│   ├── 1\n",
    "│   └── config.pbtxt\n",
    "├── postprocessing_complexity_router\n",
    "│   ├── 1\n",
    "│   │   ├── logits_processor.py\n",
    "│   │   ├── model.py\n",
    "│   │   └── __pycache__\n",
    "│   │       ├── logits_processor.cpython-310.pyc\n",
    "│   │       └── model.cpython-310.pyc\n",
    "│   └── config.pbtxt\n",
    "└── preprocessing_complexity_router\n",
    "    ├── 1\n",
    "    │   ├── model.py\n",
    "    │   └── __pycache__\n",
    "    │       └── model.cpython-310.pyc\n",
    "    └── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy the contents of `triton_template/` folder to the `/model_repository` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r triton_template/* /model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On your original machine, not within the Docker JupyterLab notebook, start the router server by running `make up`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v http://router-server:8000/v2/models/complexity_router_ensemble/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "from transformers import AutoConfig\n",
    "\n",
    "def send_request(triton_client, text):\n",
    "    input_text = np.array([[text]], dtype=object)\n",
    "    inputs = [httpclient.InferInput(\"INPUT\", input_text.shape, \"BYTES\")]\n",
    "    inputs[0].set_data_from_numpy(input_text)\n",
    "\n",
    "    outputs = [httpclient.InferRequestedOutput(\"OUTPUT\")]\n",
    "\n",
    "    response = triton_client.infer(model_name=\"complexity_router_ensemble\", inputs=inputs, outputs=outputs)\n",
    "    return response\n",
    "\n",
    "# Load the config\n",
    "config = AutoConfig.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "\n",
    "# Get complexity metrics from config\n",
    "complexity_metrics = [\n",
    "    metric for metric in config.weights_map.keys() \n",
    "    if metric != \"task_type\" and metric in config.divisor_map\n",
    "]\n",
    "\n",
    "triton_client = httpclient.InferenceServerClient(url=\"router-server:8000\")\n",
    "\n",
    "prompt = \"\"\"Prompt: Perform the task based on the input.\n",
    "\n",
    "Examples:\n",
    "Input: \"Translate the following sentence to French: 'Hello, how are you?'\"\n",
    "Output: \"Bonjour, comment ça va?\"\n",
    "\n",
    "Input: \"Summarize the following article: 'The stock market experienced a significant drop today due to global economic concerns. Analysts predict that the market will stabilize in the next few weeks.'\"\n",
    "Output: \"The stock market dropped significantly today but is expected to stabilize soon.\"\n",
    "\n",
    "Input: \"Generate a Python function that calculates the factorial of a number.\"\n",
    "Output: \n",
    "```python\n",
    "def factorial(n):  \n",
    "    if n == 0:  \n",
    "        return 1  \n",
    "    return n * factorial(n - 1)\n",
    "\"\"\"\n",
    "\n",
    "result = send_request(triton_client, prompt)\n",
    "\n",
    "# Get the complexity scores\n",
    "complexity_scores = result.as_numpy(\"OUTPUT\")\n",
    "\n",
    "# Get the one-hot encoded output\n",
    "output_data = result.as_numpy(\"OUTPUT\")\n",
    "\n",
    "# Find the index of the maximum value (which should be 1 in the one-hot vector)\n",
    "highest_complexity_index = np.argmax(output_data)\n",
    "\n",
    "# Map the index to the corresponding complexity metric\n",
    "highest_complexity = complexity_metrics[highest_complexity_index]\n",
    "\n",
    "print(f\"Input prompt: {prompt}\")\n",
    "print(f\"Highest complexity metric: {highest_complexity}\")\n",
    "print(f\"One-hot encoded output: {output_data}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
